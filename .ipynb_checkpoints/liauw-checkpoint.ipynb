{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.2.1-bin-hadoop3.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"spark-3.2.1-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAppName('Netflix').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# sc = pyspark.SparkContext(appName=\"Netflix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "combineddata*.txt is a file that contains: movie_id: user_id, rating, date\n",
    "\n",
    "c_combineddata.csv is a file that transforms combineddata.txt into movie_id, user_id, rating, date# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = StructType([\n",
    "    StructField('ID Film',IntegerType(), False),\n",
    "    StructField('ID User',IntegerType(), False),\n",
    "    StructField('Rating',IntegerType(), False),\n",
    "    StructField('Tanggal',DateType(), False)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = spark.read.csv(\n",
    "    'Cleaned Input/dataset/c_combined_data_1.csv', \n",
    "    header=True, \n",
    "    schema=data_schema\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+----------+\n",
      "|ID Film|ID User|Rating|   Tanggal|\n",
      "+-------+-------+------+----------+\n",
      "|      1| 822109|     5|2005-05-13|\n",
      "|      1| 885013|     4|2005-10-19|\n",
      "|      1|  30878|     4|2005-12-26|\n",
      "|      1| 823519|     3|2004-05-03|\n",
      "|      1| 893988|     3|2005-11-17|\n",
      "|      1| 124105|     4|2004-08-05|\n",
      "|      1|1248029|     3|2004-04-22|\n",
      "|      1|1842128|     4|2004-05-09|\n",
      "|      1|2238063|     3|2005-05-11|\n",
      "|      1|1503895|     4|2005-05-19|\n",
      "|      1|2207774|     5|2005-06-06|\n",
      "|      1|2590061|     3|2004-08-12|\n",
      "|      1|   2442|     3|2004-04-14|\n",
      "|      1| 543865|     4|2004-05-28|\n",
      "|      1|1209119|     4|2004-03-23|\n",
      "|      1| 804919|     4|2004-06-10|\n",
      "|      1|1086807|     3|2004-12-28|\n",
      "|      1|1711859|     4|2005-05-08|\n",
      "|      1| 372233|     5|2005-11-23|\n",
      "|      1|1080361|     3|2005-03-28|\n",
      "+-------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.show()\n",
    "# row = df_1.count()\n",
    "# print(f'Number of Rows {row}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+----------+\n",
      "|ID Film|ID User|Rating|   Tanggal|\n",
      "+-------+-------+------+----------+\n",
      "|   4500| 573364|     3|2005-06-20|\n",
      "|   4500|1696725|     3|2004-02-27|\n",
      "|   4500|1253431|     3|2004-03-31|\n",
      "|   4500|1265574|     2|2003-09-01|\n",
      "|   4500|1049643|     1|2003-11-15|\n",
      "|   4500|1601348|     4|2005-04-05|\n",
      "|   4500|1495289|     5|2005-07-09|\n",
      "|   4500|1254903|     3|2003-09-02|\n",
      "|   4500|2604070|     3|2005-05-15|\n",
      "|   4500|1006473|     5|2005-05-23|\n",
      "|   4500|1989892|     3|2004-04-06|\n",
      "|   4500|1517471|     4|2003-12-24|\n",
      "|   4500|1478381|     4|2005-05-21|\n",
      "|   4500| 923084|     2|2004-11-15|\n",
      "|   4500|2446292|     4|2005-10-06|\n",
      "|   4500|2554745|     3|2003-05-07|\n",
      "|   4500|1133125|     5|2004-08-10|\n",
      "|   4500| 349528|     4|2003-08-11|\n",
      "|   4500|1614895|     5|2004-08-29|\n",
      "|   4500| 424958|     4|2005-08-02|\n",
      "+-------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = spark.read.csv(\n",
    "    'Cleaned Input/dataset/c_combined_data_2.csv', \n",
    "    header=True, \n",
    "    schema=data_schema\n",
    ").cache()\n",
    "\n",
    "df_2.show()\n",
    "# row = df_2.count()\n",
    "# print(f'Number of Rows {row}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Data 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+----------+\n",
      "|ID Film|ID User|Rating|   Tanggal|\n",
      "+-------+-------+------+----------+\n",
      "|   9211|2435457|     2|2005-06-01|\n",
      "|   9211|2338545|     3|2001-02-17|\n",
      "|   9211|2218269|     1|2002-12-27|\n",
      "|   9211| 441153|     4|2002-10-11|\n",
      "|   9211|1921624|     2|2005-08-31|\n",
      "|   9211|2096652|     3|2004-05-31|\n",
      "|   9211| 818736|     2|2004-02-17|\n",
      "|   9211| 284560|     3|2003-07-27|\n",
      "|   9211|1211224|     5|2004-05-08|\n",
      "|   9211|1984086|     1|2004-09-16|\n",
      "|   9211|1389539|     3|2005-06-07|\n",
      "|   9211| 454575|     2|2004-10-23|\n",
      "|   9211| 149028|     3|2003-06-19|\n",
      "|   9211|1105843|     2|2005-04-07|\n",
      "|   9211| 353813|     2|2004-08-05|\n",
      "|   9211| 903779|     3|2001-05-21|\n",
      "|   9211| 639194|     2|2003-10-19|\n",
      "|   9211| 308031|     4|2000-09-07|\n",
      "|   9211|1794933|     1|2003-02-14|\n",
      "|   9211| 625085|     3|2004-05-21|\n",
      "+-------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3 = spark.read.csv(\n",
    "    'Cleaned Input/dataset/c_combined_data_3.csv', \n",
    "    header=True, \n",
    "    schema=data_schema\n",
    ").cache()\n",
    "\n",
    "df_3.show()\n",
    "# row = df_3.count()\n",
    "# print(f'Number of Rows {row}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Data 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+----------+\n",
      "|ID Film|ID User|Rating|   Tanggal|\n",
      "+-------+-------+------+----------+\n",
      "|  13368| 659432|     3|2005-03-16|\n",
      "|  13368| 751812|     2|2002-12-16|\n",
      "|  13368|2625420|     2|2004-05-25|\n",
      "|  13368|1650301|     1|2005-08-30|\n",
      "|  13368|2269227|     4|2005-10-27|\n",
      "|  13368|2220672|     4|2002-08-19|\n",
      "|  13368|2500511|     4|2003-08-11|\n",
      "|  13368|1452058|     2|2005-01-29|\n",
      "|  13368|1624891|     3|2002-07-27|\n",
      "|  13368| 970031|     3|2004-04-14|\n",
      "|  13368| 345673|     4|2005-04-07|\n",
      "|  13368|1426869|     5|2005-04-21|\n",
      "|  13368|1037088|     2|2005-09-14|\n",
      "|  13368|2079559|     5|2005-10-08|\n",
      "|  13368|2175560|     5|2005-01-12|\n",
      "|  13368| 946805|     4|2005-02-09|\n",
      "|  13368| 767843|     1|2005-05-04|\n",
      "|  13368| 464031|     3|2001-02-08|\n",
      "|  13368|2473764|     4|2005-09-26|\n",
      "|  13368|2339119|     3|2005-11-27|\n",
      "+-------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_4 = spark.read.csv(\n",
    "    'Cleaned Input/dataset/c_combined_data_4.csv', \n",
    "    header=True, \n",
    "    schema=data_schema\n",
    ").cache()\n",
    "\n",
    "df_4.show()\n",
    "# row = df_4.count()\n",
    "# print(f'Number of Rows {row}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+----------+\n",
      "|ID Film|ID User|Rating|   Tanggal|\n",
      "+-------+-------+------+----------+\n",
      "|      1| 822109|     5|2005-05-13|\n",
      "|      1| 885013|     4|2005-10-19|\n",
      "|      1|  30878|     4|2005-12-26|\n",
      "|      1| 823519|     3|2004-05-03|\n",
      "|      1| 893988|     3|2005-11-17|\n",
      "|      1| 124105|     4|2004-08-05|\n",
      "|      1|1248029|     3|2004-04-22|\n",
      "|      1|1842128|     4|2004-05-09|\n",
      "|      1|2238063|     3|2005-05-11|\n",
      "|      1|1503895|     4|2005-05-19|\n",
      "|      1|2207774|     5|2005-06-06|\n",
      "|      1|2590061|     3|2004-08-12|\n",
      "|      1|   2442|     3|2004-04-14|\n",
      "|      1| 543865|     4|2004-05-28|\n",
      "|      1|1209119|     4|2004-03-23|\n",
      "|      1| 804919|     4|2004-06-10|\n",
      "|      1|1086807|     3|2004-12-28|\n",
      "|      1|1711859|     4|2005-05-08|\n",
      "|      1| 372233|     5|2005-11-23|\n",
      "|      1|1080361|     3|2005-03-28|\n",
      "+-------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_samples = df_1.union(df_2.union(df_3.union(df_4)))\n",
    "all_samples.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_samples.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cek data apakah ada yang kosong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+----------+\n",
      "|ID Film|ID User|Rating|   Tanggal|\n",
      "+-------+-------+------+----------+\n",
      "|      1| 822109|     5|2005-05-13|\n",
      "|      1| 885013|     4|2005-10-19|\n",
      "|      1|  30878|     4|2005-12-26|\n",
      "|      1| 823519|     3|2004-05-03|\n",
      "|      1| 893988|     3|2005-11-17|\n",
      "|      1| 124105|     4|2004-08-05|\n",
      "|      1|1248029|     3|2004-04-22|\n",
      "|      1|1842128|     4|2004-05-09|\n",
      "|      1|2238063|     3|2005-05-11|\n",
      "|      1|1503895|     4|2005-05-19|\n",
      "|      1|2207774|     5|2005-06-06|\n",
      "|      1|2590061|     3|2004-08-12|\n",
      "|      1|   2442|     3|2004-04-14|\n",
      "|      1| 543865|     4|2004-05-28|\n",
      "|      1|1209119|     4|2004-03-23|\n",
      "|      1| 804919|     4|2004-06-10|\n",
      "|      1|1086807|     3|2004-12-28|\n",
      "|      1|1711859|     4|2005-05-08|\n",
      "|      1| 372233|     5|2005-11-23|\n",
      "|      1|1080361|     3|2005-03-28|\n",
      "+-------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_samples.show()\n",
    "#all_samples[['ID Film', 'ID User', 'Rating', 'Tanggal']].summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidak ada data yang kosong\n",
    "Jumlah data ada 100.480.162\n",
    "Data diambil dari 1999-11-11 hingga 2005-12-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "added_column = all_samples.withColumn('year', year(all_samples['Tanggal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year_list = range(1999,2006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_count = []\n",
    "# for y in year_list:\n",
    "#     user_count.append(added_column.select('ID User').where(added_column.year == y).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data Training dan Testing\n",
    "## Tahun 1999 - 2004 Training, 2005 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_dataset_1 = added_column.select('ID User','ID Film','Rating').where(added_column.year <= 2004)\n",
    "\n",
    "# dataset_2005 = added_column.select('ID User','ID Film','Rating').where(added_column.year == 2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql.window import *\n",
    "\n",
    "# dataset_2005 = dataset_2005.orderBy('Tanggal').withColumn(\"monotonically_increasing_id\", monotonically_increasing_id())\n",
    "# window = Window.orderBy(col('monotonically_increasing_id'))\n",
    "# dataset_2005 = dataset_2005.withColumn('id', row_number().over(window))\n",
    "\n",
    "# training_dataset_2 = dataset_2005.select('ID User','ID Film','Rating').where(dataset_2005.id <= 20000000)\n",
    "\n",
    "# training_dataset = training_dataset_1.union(training_dataset_2)\n",
    "# testing_dataset = dataset_2005.select('ID User','ID Film','Rating').where(dataset_2005.id > 20000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_1 = added_column.select('ID User','ID Film','Rating').where(added_column.year <= 2004)\n",
    "\n",
    "dataset_2005 = added_column.select('ID User','ID Film','Rating').where(added_column.year == 2005).orderBy('Tanggal')\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "dataset_2005 = dataset_2005.orderBy('Tanggal')\n",
    "dataset_2005 = dataset_2005.withColumn(\"monotonically_increasing_id\", monotonically_increasing_id())\n",
    "\n",
    "\n",
    "window = Window.orderBy(col('monotonically_increasing_id'))\n",
    "dataset_2005 = dataset_2005.withColumn('id', row_number().over(window))\n",
    "\n",
    "dataset_bonus = dataset_2005.select('ID User','ID Film','Rating').where(dataset_2005.id <= 33000000)\n",
    "\n",
    "training_dataset = training_dataset_1.union(dataset_bonus)\n",
    "testing_dataset = dataset_2005.select('ID User','ID Film','Rating').where(dataset_2005.id > 33000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80230209"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20249953"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_dataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing_dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(training_dataset, maxIter=5, regParam=0.1, rank=10, numItemBlocks=15, numUserBlocks=15, alpha=1):\n",
    "    als = ALS(maxIter=maxIter, \n",
    "              regParam=regParam, \n",
    "              userCol=\"ID User\", \n",
    "              itemCol=\"ID Film\", \n",
    "              ratingCol=\"Rating\",\n",
    "              rank=rank,\n",
    "              numItemBlocks=numItemBlocks,\n",
    "              numUserBlocks = numUserBlocks,\n",
    "              alpha = alpha,\n",
    "              nonnegative = True, \n",
    "              coldStartStrategy=\"drop\",\n",
    "             implicitPrefs=False)\n",
    "    model = als.fit(training_dataset)\n",
    "    return model,als"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,testing_dataset):\n",
    "    predictions = model.transform(testing_dataset)\n",
    "    evaluator = RegressionEvaluator(\n",
    "                        metricName=\"rmse\", \n",
    "                        labelCol=\"Rating\",\n",
    "                        predictionCol=\"prediction\")\n",
    "\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,als = training_model(training_dataset, \n",
    "                       maxIter=15, \n",
    "                       regParam=0.08, \n",
    "                       rank=20, \n",
    "                       numItemBlocks=30, \n",
    "                       numUserBlocks=30, \n",
    "                       alpha=0.95 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.9420246259113683\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model,testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|ID User|     recommendations|\n",
      "+-------+--------------------+\n",
      "|    296|[{13635, 4.637179...|\n",
      "|    392|[{13684, 3.991778...|\n",
      "|    471|[{13635, 5.017137...|\n",
      "|    481|[{9599, 5.680025}...|\n",
      "|    540|[{13635, 6.045090...|\n",
      "|    623|[{14630, 4.799587...|\n",
      "|    633|[{9599, 4.990544}...|\n",
      "|    857|[{13504, 4.901835...|\n",
      "|   1088|[{13504, 4.855355...|\n",
      "|   1522|[{2942, 5.197427}...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = model.recommendForAllUsers(10)\n",
    "best_model.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+\n",
      "|ID User|ID Film|   Rating|\n",
      "+-------+-------+---------+\n",
      "|    296|  13635| 4.637179|\n",
      "|    296|    933| 4.622764|\n",
      "|    296|  10316|4.5790715|\n",
      "|    296|     32|4.5607915|\n",
      "|    296|   7252| 4.540528|\n",
      "|    296|  16566|4.5050397|\n",
      "|    296|   4048| 4.501928|\n",
      "|    296|  16886| 4.501011|\n",
      "|    296|   8970|4.4972596|\n",
      "|    296|  17182|  4.49337|\n",
      "+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nrecommendations = best_model\\\n",
    "    .withColumn(\"rec_exp\", explode(\"recommendations\"))\\\n",
    "    .select('ID User', col(\"rec_exp.ID Film\"), col(\"rec_exp.Rating\"))\n",
    "\n",
    "nrecommendations.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+\n",
      "|ID Film|year|          Movie Name|\n",
      "+-------+----+--------------------+\n",
      "|      1|2003|     Dinosaur Planet|\n",
      "|      2|2004|Isle of Man TT 20...|\n",
      "|      3|1997|           Character|\n",
      "|      4|1994|Paula Abdul's Get...|\n",
      "|      5|2004|The Rise and Fall...|\n",
      "|      6|1997|                Sick|\n",
      "|      7|1992|               8 Man|\n",
      "|      8|2004|What the #$*! Do ...|\n",
      "|      9|1991|Class of Nuke 'Em...|\n",
      "|     10|2001|             Fighter|\n",
      "|     11|1999|Full Frame: Docum...|\n",
      "|     12|1947|My Favorite Brunette|\n",
      "|     13|2003|Lord of the Rings...|\n",
      "|     14|1982|  Nature: Antarctica|\n",
      "|     15|1988|Neil Diamond: Gre...|\n",
      "|     16|1996|           Screamers|\n",
      "|     17|2005|           7 Seconds|\n",
      "|     18|1994|    Immortal Beloved|\n",
      "|     19|2000|By Dawn's Early L...|\n",
      "|     20|1972|     Seeta Aur Geeta|\n",
      "+-------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_schema = StructType([\n",
    "    StructField('ID Film',IntegerType(), False),\n",
    "    StructField('year',IntegerType(), False),\n",
    "    StructField('Movie Name',StringType(), False),\n",
    "])\n",
    "\n",
    "movies = spark.read.csv(\n",
    "    'movie_titles.csv', \n",
    "    header=True, \n",
    "    schema=data_schema\n",
    ").cache()\n",
    "\n",
    "movies.show()\n",
    "\n",
    "# from pyspark.ml.recommendation import ALSModel\n",
    "\n",
    "# model2 = ALSModel.load('ModelSaveOut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+----+--------------------+\n",
      "|ID Film|ID User|   Rating|year|          Movie Name|\n",
      "+-------+-------+---------+----+--------------------+\n",
      "|  13504|    857|4.9018354|2004|               House|\n",
      "|  14818|    857|4.8458295|1993|     Chef!: Series 1|\n",
      "|   9864|    857| 4.825719|2004|Battlestar Galact...|\n",
      "|   8081|    857| 4.820572|1994|Knowing Me Knowin...|\n",
      "|   6725|    857|4.8088875|1979|       Connections 3|\n",
      "|   5494|    857|4.8088875|1979|       Connections 2|\n",
      "|  13922|    857|4.8041487|2003|Little Britain: S...|\n",
      "|  10065|    857| 4.785265|2005|    Nina's Tragedies|\n",
      "|  13635|    857|4.7820425|2001|The Amazing Race:...|\n",
      "|   3771|    857| 4.763016|1974|   Kung Fu: Season 3|\n",
      "+-------+-------+---------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nrecommendations.join(movies, on='ID Film').filter('`ID User`=857').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1912.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter.saveImpl(ReadWrite.scala:384)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$3(SparkHadoopWriter.scala:100)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\t... 51 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-ffdc72c774e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path should be a string, got type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1912.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter.saveImpl(ReadWrite.scala:384)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$3(SparkHadoopWriter.scala:100)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\t... 51 more\r\n"
     ]
    }
   ],
   "source": [
    "als.save('best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# als = ALS(\n",
    "#          userCol=\"ID User\", \n",
    "#          itemCol=\"ID Film\",\n",
    "#          ratingCol=\"Rating\", \n",
    "#          nonnegative = True, \n",
    "#          implicitPrefs = False,\n",
    "#          coldStartStrategy=\"drop\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add hyperparameters and their respective values to param_grid\n",
    "# param_grid = ParamGridBuilder() \\\n",
    "#             .addGrid(als.maxIter, [5, 10, 15]) \\\n",
    "#             .addGrid(als.rank, [10, 50, 100, 150]) \\\n",
    "#             .addGrid(als.regParam, [.1, .15]) \\\n",
    "#             .addGrid(als.numItemBlocks, [10, 20, 50]) \\\n",
    "#             .addGrid(als.numUserBlocks, [10, 20, 50]) \\\n",
    "#             .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define evaluator as RMSE and print length of evaluator\n",
    "# evaluator = RegressionEvaluator(\n",
    "#            metricName=\"rmse\", \n",
    "#            labelCol=\"Rating\", \n",
    "#            predictionCol=\"prediction\") \n",
    "# print (\"Num models to be tested: \", len(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cross validation using CrossValidator\n",
    "# cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Fit cross validator to the 'train' dataset\n",
    "# model = cv.fit(training_dataset)\n",
    "# #Extract best model from the cv model above\n",
    "# best_model = model.bestModel\n",
    "# # View the predictions\n",
    "# test_predictions = best_model.transform(testing_dataset)\n",
    "# RMSE = evaluator.evaluate(test_predictions)\n",
    "# print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"**Best Model**\")\n",
    "# # Print \"Rank\"\n",
    "# print(\"  Rank:\", best_model._java_obj.parent().getRank())\n",
    "# # Print \"MaxIter\"\n",
    "# print(\"  MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
    "# # Print \"RegParam\"\n",
    "# print(\"  RegParam:\", best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
